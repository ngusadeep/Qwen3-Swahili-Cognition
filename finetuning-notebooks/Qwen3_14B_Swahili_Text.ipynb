{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngusadeep/Qwen3-Swahili-Cognition/blob/main/finetuning-notebooks/Qwen3_14B_Swahili_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCuBW7g-tL7a"
      },
      "source": [
        "# **Fine-tuning Qwen3 LLM for Enhanced Swahili Cognitive Capabilities**\n",
        "## Pushing the frontier of Swahili language understanding across Text, Speech, and Vision\n",
        "\n",
        "This notebook fine-tunes the Qwen3 LLM suite to enhance its Swahili cognitive capabilities using the **Swahili-Instructions** dataset from Kaggle. This dataset contains high-quality Swahili instruction-following examples that improve the model's conversational and instruction-following abilities in Swahili.\n",
        "\n",
        "## Framework: Unsloth for Efficient Fine-tuning\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSkp1naWtL7e"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5617pnLhtL7e"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2\n",
        "!pip install --quiet kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# HUGGINGFACE TOKEN SETUP (Using Google Colab userdata)\n",
        "# =============================================================================\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    HF_TOKEN = userdata.get('HF_API_KEY')\n",
        "    print(\"‚úÖ Successfully loaded HuggingFace token from Colab userdata\")\n",
        "except:\n",
        "    HF_TOKEN = None\n",
        "    print(\"‚ö†Ô∏è Could not load HF_API_KEY from Colab userdata. Set it in Colab secrets if you need to use gated models or push to hub.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iajq1W8ipjyK"
      },
      "source": [
        "### Load Qwen3 Model with Unsloth\n",
        "\n",
        "We'll load the Qwen3-14B model using Unsloth's FastLanguageModel for efficient fine-tuning. The model will be loaded in 4-bit quantization to optimize memory usage in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "fourbit_models = [\n",
        "    \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\", # Qwen 14B 2x faster\n",
        "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-32B-unsloth-bnb-4bit\",\n",
        "\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Phi-4\",\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit\" # [NEW] We support TTS models!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-14B\",\n",
        "    max_seq_length = 2048,   # Context length - can be longer, but uses more memory\n",
        "    load_in_4bit = True,     # 4bit uses much less memory\n",
        "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # We have full finetuning now!\n",
        "    token = HF_TOKEN if HF_TOKEN else None,  # Use token from Colab userdata if available\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "### Add LoRA Adapters for Efficient Fine-tuning\n",
        "\n",
        "We now add LoRA (Low-Rank Adaptation) adapters so we only need to update 1 to 10% of all parameters! This makes fine-tuning much more memory-efficient while maintaining model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,  # Best to choose alpha = rank or rank*2\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,   # We support rank stabilized LoRA\n",
        "    loftq_config = None,  # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Test\"></a>\n",
        "### Test Model with Swahili Prompt (Before Fine-tuning)\n",
        "\n",
        "Let's first test the base model's Swahili capabilities before fine-tuning to establish a baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kyTw2n1edte"
      },
      "outputs": [],
      "source": [
        "# Test the base model with a Swahili prompt\n",
        "messages = [\n",
        "    {\"role\" : \"user\", \"content\" : \"Eleza kwa ufupi kuhusu historia ya Kiswahili na umuhimu wake katika Afrika Mashariki.\"}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True,\n",
        "    enable_thinking = False,\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "print(\"=== Base Model Response (Before Fine-tuning) ===\")\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 256,\n",
        "    temperature = 0.7, top_p = 0.8, top_k = 20,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTZICZtie3lQ"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Preparation for Swahili Fine-tuning\n",
        "\n",
        "We'll use the **Swahili-Instructions** dataset from Kaggle which contains high-quality Swahili instruction-following examples. This dataset is in instruction/input/output format and we'll convert it to ShareGPT-style conversations for training.\n",
        "\n",
        "Let's download and prepare the Swahili dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjgH3lt0e2Sz"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# BLOCK 1: IMPORTS AND SETUP\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import kagglehub\n",
        "from datasets import Dataset\n",
        "\n",
        "print(\"‚úì All required libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYR9DISZgyA6"
      },
      "source": [
        "### Download Swahili-Instructions Dataset from Kaggle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zoaygOAe3I2"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# BLOCK 2: DIRECTORY STRUCTURE SETUP\n",
        "# =============================================================================\n",
        "\n",
        "# Create directories for organized storage\n",
        "DATA_DIR = Path(\"./datasets\")\n",
        "KAGGLE_DIR = DATA_DIR / \"kaggle\"\n",
        "\n",
        "# Create directories\n",
        "for dir_path in [DATA_DIR, KAGGLE_DIR]:\n",
        "    dir_path.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "print(\"üìÅ Created directory structure:\")\n",
        "print(f\"   Main data directory: {DATA_DIR}\")\n",
        "print(f\"   Kaggle datasets: {KAGGLE_DIR}\")\n",
        "print(\"‚úÖ Directory setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX8H3urDe00l"
      },
      "source": [
        "### Download Swahili-Instructions Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbh19fTOfHDB"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# BLOCK 3: KAGGLE DOWNLOAD HELPER FUNCTION\n",
        "# =============================================================================\n",
        "\n",
        "def download_kaggle_with_hub(dataset_id, local_name):\n",
        "    \"\"\"Download Kaggle dataset using kagglehub\"\"\"\n",
        "    try:\n",
        "        print(f\"üì• Downloading {dataset_id}...\")\n",
        "        dataset_path = kagglehub.dataset_download(dataset_id)\n",
        "        print(f\"‚úÖ Downloaded to: {dataset_path}\")\n",
        "\n",
        "        # Create symlink or copy to our organized structure\n",
        "        target_path = KAGGLE_DIR / local_name\n",
        "        if target_path.exists():\n",
        "            import shutil\n",
        "            shutil.rmtree(target_path)\n",
        "\n",
        "        # Create symlink for efficient access\n",
        "        try:\n",
        "            target_path.symlink_to(Path(dataset_path), target_is_directory=True)\n",
        "            print(f\"üîó Linked to: {target_path}\")\n",
        "        except OSError:\n",
        "            # If symlink fails, copy the data\n",
        "            import shutil\n",
        "            shutil.copytree(dataset_path, target_path)\n",
        "            print(f\"üìÅ Copied to: {target_path}\")\n",
        "\n",
        "        return str(dataset_path), target_path\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error downloading {dataset_id}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "print(\"‚úÖ Kaggle download helper function ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTexROzQfJn5"
      },
      "source": [
        "### Download Swahili-Instructions Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkj4c6NrfIz3"
      },
      "outputs": [],
      "source": [
        "print(\"üöÄ Starting Kaggle dataset download...\")\n",
        "\n",
        "# Download Swahili Instructions Dataset\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üìö SWAHILI INSTRUCTIONS DATASET\")\n",
        "print(\"=\"*50)\n",
        "swahili_instructions_path, swahili_instructions_dir = download_kaggle_with_hub(\n",
        "    \"alfaxadeyembe/swahili-instructions\", \"swahili_instructions\"\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Kaggle dataset download completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OMhyEXkfM5e"
      },
      "source": [
        "### Load and Examine Swahili-Instructions Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXBFaeQHfSxp"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# BLOCK 4: LOAD SWAHILI INSTRUCTIONS DATA\n",
        "# =============================================================================\n",
        "\n",
        "print(\"üìñ Reading Swahili Instructions Dataset...\")\n",
        "\n",
        "# Load the JSON file\n",
        "swahili_instructions_path = Path(\"datasets/kaggle/swahili_instructions\")\n",
        "json_files = list(swahili_instructions_path.glob(\"*.json\"))\n",
        "\n",
        "if json_files:\n",
        "    json_file = json_files[0]\n",
        "    print(f\"‚úÖ Found JSON file: {json_file}\")\n",
        "\n",
        "    # Read the JSON data\n",
        "    with open(json_file, 'r', encoding='utf-8') as f:\n",
        "        swahili_data = json.load(f)\n",
        "\n",
        "    print(f\"‚úÖ Successfully loaded JSON data\")\n",
        "    print(f\"üìä Data type: {type(swahili_data)}\")\n",
        "\n",
        "    # If it's a list, show first few items\n",
        "    if isinstance(swahili_data, list):\n",
        "        print(f\"üìà Number of entries: {len(swahili_data)}\")\n",
        "        print(f\"\\nüìã First entry structure:\")\n",
        "        if swahili_data:\n",
        "            first_item = swahili_data[0]\n",
        "            print(f\"   Keys: {list(first_item.keys()) if isinstance(first_item, dict) else 'Not a dict'}\")\n",
        "            print(f\"   Sample: {first_item}\")\n",
        "\n",
        "            # Show a few more samples to understand the structure\n",
        "            print(f\"\\nüìù Sample entries (first 3):\")\n",
        "            for i, item in enumerate(swahili_data[:3]):\n",
        "                print(f\"   Entry {i+1}: {item}\")\n",
        "\n",
        "    # If it's a dict, show the structure\n",
        "    elif isinstance(swahili_data, dict):\n",
        "        print(f\"üìö Dictionary keys: {list(swahili_data.keys())}\")\n",
        "        for key, value in swahili_data.items():\n",
        "            print(f\"   {key}: {type(value)} - {len(value) if hasattr(value, '__len__') else value}\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No JSON files found in the directory\")\n",
        "    swahili_data = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9FcosGvfdNr"
      },
      "source": [
        "### Convert to ShareGPT Format and Standardize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7Y0OonwVrWi"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# BLOCK 5: STANDARDIZE DATA FORMATS USING UNSLOTH\n",
        "# =============================================================================\n",
        "\n",
        "from unsloth.chat_templates import standardize_data_formats\n",
        "\n",
        "print(\"üîÑ Converting Swahili instructions to standardized format...\")\n",
        "\n",
        "# Convert the raw JSON data to ShareGPT format first\n",
        "def convert_to_conversations_format(data):\n",
        "    \"\"\"Convert instruction-input-output format to conversations format\"\"\"\n",
        "    conversations_data = []\n",
        "\n",
        "    for item in data:\n",
        "        # Create the user message\n",
        "        user_message = item['instruction']\n",
        "\n",
        "        # If there's input content, add it to the instruction\n",
        "        if item.get('input', '').strip():\n",
        "            user_message += f\"\\n\\n{item['input']}\"\n",
        "\n",
        "        # Create conversation structure\n",
        "        conversation = {\n",
        "            \"conversations\": [\n",
        "                {\"from\": \"human\", \"value\": user_message},\n",
        "                {\"from\": \"gpt\", \"value\": item['output']}\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # Keep the ID for reference\n",
        "        if 'id' in item:\n",
        "            conversation['id'] = item['id']\n",
        "\n",
        "        conversations_data.append(conversation)\n",
        "\n",
        "    return conversations_data\n",
        "\n",
        "# Convert to conversations format\n",
        "print(f\"üìù Converting {len(swahili_data)} entries to conversations format...\")\n",
        "conversations_data = convert_to_conversations_format(swahili_data)\n",
        "\n",
        "# Create HuggingFace dataset\n",
        "dataset = Dataset.from_list(conversations_data)\n",
        "print(f\"‚úÖ Created dataset with {len(dataset)} conversations\")\n",
        "\n",
        "# Apply unsloth's standardize_data_formats\n",
        "print(\"üîß Applying unsloth's standardize_data_formats...\")\n",
        "try:\n",
        "    dataset = standardize_data_formats(dataset)\n",
        "    print(\"‚úÖ Data standardization completed!\")\n",
        "\n",
        "    # Print sample to see the structure after standardization\n",
        "    print(f\"\\nüìä Sample after standardization:\")\n",
        "    print(f\"Dataset features: {dataset.features}\")\n",
        "\n",
        "    if len(dataset) > 0:\n",
        "        sample = dataset[0]\n",
        "        print(f\"\\nüìã Sample entry structure:\")\n",
        "        for key, value in sample.items():\n",
        "            if isinstance(value, list) and len(value) > 0:\n",
        "                print(f\"  {key}: {value}\")\n",
        "            elif isinstance(value, str) and len(value) > 200:\n",
        "                print(f\"  {key}: {value[:200]}...\")\n",
        "            else:\n",
        "                print(f\"  {key}: {value}\")\n",
        "\n",
        "        print(f\"\\nüí¨ Sample conversation:\")\n",
        "        if 'conversations' in sample:\n",
        "            for i, turn in enumerate(sample['conversations']):\n",
        "                # Handle the new format: role/content instead of from/value\n",
        "                role = turn.get('role', turn.get('from', 'unknown'))\n",
        "                content = turn.get('content', turn.get('value', 'no content'))\n",
        "                print(f\"  Turn {i+1} ({role}): {content[:150]}...\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Dataset ready for chat template application!\")\n",
        "    print(f\"   Total conversations: {len(dataset)}\")\n",
        "    print(f\"   Features: {list(dataset.features.keys())}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error during standardization: {e}\")\n",
        "    print(\"Dataset created but standardization failed. You may proceed with manual formatting.\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb0hbEekfeqf"
      },
      "outputs": [],
      "source": [
        "### Apply Qwen3 Chat Template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_0L18QMfot4"
      },
      "source": [
        "Now we'll apply the Qwen3 chat template to format the conversations for training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unDFuUq1foWj"
      },
      "outputs": [],
      "source": [
        "# Apply Qwen3 chat template to the dataset\n",
        "print(\"üé® Applying Qwen3 chat template...\")\n",
        "conversations_text = tokenizer.apply_chat_template(\n",
        "    list(dataset[\"conversations\"]),\n",
        "    tokenize = False,\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Applied chat template to {len(conversations_text)} conversations\")\n",
        "print(f\"\\nüìù Sample formatted conversation (first 500 chars):\")\n",
        "print(conversations_text[0][:500] + \"...\" if len(conversations_text[0]) > 500 else conversations_text[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgknnOf7fn3e"
      },
      "source": [
        "### Create Final Training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_szfriCBgCkU"
      },
      "outputs": [],
      "source": [
        "# Convert to pandas Series and create final dataset\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.Series(conversations_text)\n",
        "data.name = \"text\"\n",
        "\n",
        "# Create HuggingFace dataset\n",
        "combined_dataset = Dataset.from_pandas(pd.DataFrame(data))\n",
        "combined_dataset = combined_dataset.shuffle(seed = 3407)\n",
        "\n",
        "print(f\"‚úÖ Final training dataset created!\")\n",
        "print(f\"   Total examples: {len(combined_dataset)}\")\n",
        "print(f\"   Dataset features: {combined_dataset.features}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DANuEJA7gL58"
      },
      "source": [
        "### Preview Final Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-e0KO9GgFy3"
      },
      "outputs": [],
      "source": [
        "# Preview a sample from the final dataset\n",
        "print(\"üìñ Sample from final training dataset:\")\n",
        "print(\"=\"*70)\n",
        "sample_text = combined_dataset[0][\"text\"]\n",
        "print(sample_text[:800] + \"...\" if len(sample_text) > 800 else sample_text)\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR-4prS_gVel"
      },
      "source": [
        "The dataset is now ready for training! We have successfully:\n",
        "1. ‚úÖ Downloaded the Swahili-Instructions dataset from Kaggle\n",
        "2. ‚úÖ Converted instruction/input/output format to ShareGPT conversations format\n",
        "3. ‚úÖ Standardized the data using Unsloth's `standardize_data_formats`\n",
        "4. ‚úÖ Applied Qwen3 chat template\n",
        "5. ‚úÖ Created final training dataset with shuffled examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfV47_SXgXH4"
      },
      "outputs": [],
      "source": [
        "# Dataset is ready! The combined_dataset variable is already created above.\n",
        "# You can now proceed to training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Fine-tune Qwen3 for Swahili\n",
        "\n",
        "Now let's train our model on the Swahili datasets. We'll do 30 steps initially for testing, but you can set `num_train_epochs=1` for a full training run, and set `max_steps=None` to disable step limiting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = combined_dataset,\n",
        "    eval_dataset = None, # Can set up evaluation!\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 30,\n",
        "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.001,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"none\", # Use TrackIO/WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9fa371ShyhB"
      },
      "source": [
        "### Start Swahili Fine-tuning\n",
        "\n",
        "Let's begin training the model on Swahili data! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Test Fine-tuned Model with Swahili Prompts\n",
        "\n",
        "Let's test our fine-tuned model with Swahili prompts! According to the Qwen-3 team, the recommended settings for reasoning inference are `temperature = 0.6, top_p = 0.95, top_k = 20`. For normal chat-based inference, use `temperature = 0.7, top_p = 0.8, top_k = 20`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "# Test with Swahili conversational prompt (non-thinking mode)\n",
        "messages = [\n",
        "    {\"role\" : \"user\", \"content\" : \"Eleza kwa ufupi kuhusu umuhimu wa lugha ya Kiswahili katika Afrika Mashariki na jinsi inavyotumika katika maisha ya kila siku.\"}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    enable_thinking = False, # Disable thinking\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "print(\"=== Fine-tuned Model Response (Swahili - Non-Thinking Mode) ===\")\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 256, # Increase for longer outputs!\n",
        "    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j873RMcEi9uq"
      },
      "outputs": [],
      "source": [
        "# Test with Swahili reasoning prompt (thinking mode enabled)\n",
        "messages = [\n",
        "    {\"role\" : \"user\", \"content\" : \"Kama una shilingi 5000 na unataka kununua vitabu viwili: moja ni shilingi 2000 na nyingine ni shilingi 3500. Je, utabaki na pesa ngapi?\"}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    enable_thinking = True, # Enable thinking for reasoning\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "print(\"\\n=== Fine-tuned Model Response (Swahili - Thinking Mode) ===\")\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 1024, # Increase for longer outputs!\n",
        "    temperature = 0.6, top_p = 0.95, top_k = 20, # For thinking\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Save Fine-tuned Swahili Model\n",
        "\n",
        "To save the final fine-tuned Swahili model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = HF_TOKEN) # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = HF_TOKEN) # Online saving\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "### Load Saved Swahili Model\n",
        "\n",
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = 2048,\n",
        "        load_in_4bit = True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Save Swahili Model in Different Formats\n",
        "\n",
        "We also support saving to `float16` directly for VLLM deployment. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! The token is automatically loaded from Colab userdata (HF_API_KEY)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = HF_TOKEN)\n",
        "\n",
        "# Merge to 4bit\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = HF_TOKEN)\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained(\"model\")\n",
        "    tokenizer.save_pretrained(\"model\")\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub(\"hf/model\", token = HF_TOKEN)\n",
        "    tokenizer.push_to_hub(\"hf/model\", token = HF_TOKEN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### Export Swahili Model to GGUF / llama.cpp Format\n",
        "\n",
        "To save your fine-tuned Swahili model to `GGUF` / `llama.cpp` format, we support it natively! We clone `llama.cpp` and default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Token is loaded from Colab userdata (HF_API_KEY)\n",
        "# And change hf to your username!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, token = HF_TOKEN)\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = HF_TOKEN)\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = HF_TOKEN)\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = HF_TOKEN, # Token loaded from Colab userdata (HF_API_KEY)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOfJSxs_VJjz"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp.\n",
        "\n",
        "## Summary\n",
        "\n",
        "We've successfully fine-tuned the Qwen3-14B model for enhanced Swahili cognitive capabilities using the Nadhari/Swahili-Thinking datasets. The model now has improved:\n",
        "- **Text understanding** in Swahili\n",
        "- **Reasoning capabilities** for Swahili problem-solving\n",
        "- **Conversational abilities** in Swahili\n",
        "\n",
        "The fine-tuned model can be deployed for various Swahili language applications across African countries and globally.\n",
        "\n",
        "---\n",
        "\n",
        "## Resources and Support\n",
        "\n",
        "If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other useful links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "4. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
        "</div>\n",
        "\n",
        "  This notebook and all Unsloth notebooks are licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
