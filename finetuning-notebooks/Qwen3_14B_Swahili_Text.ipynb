{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngusadeep/Qwen3-Swahili-Cognition/blob/main/finetuning-notebooks/Qwen3_14B_Swahili_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCuBW7g-tL7a"
      },
      "source": [
        "# **Fine-tuning Qwen3 LLM for Enhanced Swahili Cognitive Capabilities**\n",
        "## Pushing the frontier of Swahili language understanding across Text, Speech, and Vision\n",
        "\n",
        "This notebook fine-tunes the Qwen3 LLM suite to enhance its Swahili cognitive capabilities using diverse datasets tailored for the Swahili language across African countries and globally. We utilize the **Nadhari/Swahili-Thinking** datasets to improve the model's reasoning and conversational abilities in Swahili.\n",
        "\n",
        "## Framework: Unsloth for Efficient Fine-tuning\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSkp1naWtL7e"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5617pnLhtL7e"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iajq1W8ipjyK"
      },
      "source": [
        "### Load Qwen3 Model with Unsloth\n",
        "\n",
        "We'll load the Qwen3-14B model using Unsloth's FastLanguageModel for efficient fine-tuning. The model will be loaded in 4-bit quantization to optimize memory usage in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "fourbit_models = [\n",
        "    \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\", # Qwen 14B 2x faster\n",
        "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-32B-unsloth-bnb-4bit\",\n",
        "\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Phi-4\",\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit\" # [NEW] We support TTS models!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-14B\",\n",
        "    max_seq_length = 2048,   # Context length - can be longer, but uses more memory\n",
        "    load_in_4bit = True,     # 4bit uses much less memory\n",
        "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # We have full finetuning now!\n",
        "    # token = \"hf_...\",      # use one if using gated models\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "### Add LoRA Adapters for Efficient Fine-tuning\n",
        "\n",
        "We now add LoRA (Low-Rank Adaptation) adapters so we only need to update 1 to 10% of all parameters! This makes fine-tuning much more memory-efficient while maintaining model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,  # Best to choose alpha = rank or rank*2\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,   # We support rank stabilized LoRA\n",
        "    loftq_config = None,  # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Test\"></a>\n",
        "### Test Model with Swahili Prompt (Before Fine-tuning)\n",
        "\n",
        "Let's first test the base model's Swahili capabilities before fine-tuning to establish a baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kyTw2n1edte"
      },
      "outputs": [],
      "source": [
        "# Test the base model with a Swahili prompt\n",
        "messages = [\n",
        "    {\"role\" : \"user\", \"content\" : \"Eleza kwa ufupi kuhusu historia ya Kiswahili na umuhimu wake katika Afrika Mashariki.\"}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True,\n",
        "    enable_thinking = False,\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "print(\"=== Base Model Response (Before Fine-tuning) ===\")\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 256,\n",
        "    temperature = 0.7, top_p = 0.8, top_k = 20,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTZICZtie3lQ"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Preparation for Swahili Fine-tuning\n",
        "\n",
        "We'll use the **Nadhari/Swahili-Thinking** datasets which contain Swahili reasoning and conversational data. Qwen3 supports both reasoning and non-reasoning modes, so we'll prepare the datasets accordingly:\n",
        "\n",
        "1. **Swahili-Thinking dataset**: Contains Swahili reasoning traces and problem-solving examples\n",
        "2. **Swahili conversational data**: Contains multi-turn Swahili conversations in ShareGPT format\n",
        "\n",
        "Let's load the Swahili datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjgH3lt0e2Sz"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load Swahili-Thinking dataset for reasoning capabilities\n",
        "try:\n",
        "    reasoning_dataset = load_dataset(\"Nadhari/Swahili-Thinking\", split = \"train\")\n",
        "    print(\"✓ Loaded Nadhari/Swahili-Thinking dataset\")\n",
        "except:\n",
        "    # Fallback if dataset structure is different\n",
        "    reasoning_dataset = load_dataset(\"Nadhari/Swahili-Thinking\")\n",
        "    if isinstance(reasoning_dataset, dict):\n",
        "        reasoning_dataset = reasoning_dataset.get(\"train\", list(reasoning_dataset.values())[0])\n",
        "    print(\"✓ Loaded Nadhari/Swahili-Thinking dataset (alternative format)\")\n",
        "\n",
        "# Check if there's a conversational split or use the same dataset\n",
        "try:\n",
        "    non_reasoning_dataset = load_dataset(\"Nadhari/Swahili-Thinking\", split = \"chat\")\n",
        "    print(\"✓ Loaded conversational split\")\n",
        "except:\n",
        "    # If no separate chat split, we'll use a subset of the main dataset\n",
        "    non_reasoning_dataset = None\n",
        "    print(\"ℹ Using same dataset for both reasoning and conversational data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's examine the structure of the Swahili datasets:\n"
      ],
      "metadata": {
        "id": "zYR9DISZgyA6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zoaygOAe3I2"
      },
      "outputs": [],
      "source": [
        "print(\"Reasoning Dataset Structure:\")\n",
        "print(reasoning_dataset)\n",
        "print(f\"\\nNumber of examples: {len(reasoning_dataset)}\")\n",
        "if len(reasoning_dataset) > 0:\n",
        "    print(\"\\nFirst example keys:\", reasoning_dataset[0].keys())\n",
        "    print(\"\\nFirst example preview:\")\n",
        "    for key, value in list(reasoning_dataset[0].items())[:3]:\n",
        "        print(f\"  {key}: {str(value)[:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX8H3urDe00l"
      },
      "source": [
        "### Convert Swahili Datasets to Conversational Format\n",
        "\n",
        "We need to convert the Swahili-Thinking dataset into a conversational format that Qwen3 can understand. The exact field names may vary, so we'll handle different dataset structures:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbh19fTOfHDB"
      },
      "outputs": [],
      "source": [
        "def generate_conversation(examples):\n",
        "    \"\"\"Convert Swahili dataset examples to conversational format\"\"\"\n",
        "    conversations = []\n",
        "\n",
        "    # Get the batch size - determine from the first key that has a list\n",
        "    batch_size = None\n",
        "    for key in examples.keys():\n",
        "        if isinstance(examples[key], list):\n",
        "            batch_size = len(examples[key])\n",
        "            break\n",
        "\n",
        "    if batch_size is None:\n",
        "        # Single example case\n",
        "        batch_size = 1\n",
        "\n",
        "    # Try different possible field names for Swahili-Thinking dataset\n",
        "    # Common patterns: problem/solution, question/answer, instruction/response, etc.\n",
        "    for i in range(batch_size):\n",
        "        # Extract single example from batch\n",
        "        example = {}\n",
        "        for key in examples.keys():\n",
        "            if isinstance(examples[key], list):\n",
        "                example[key] = examples[key][i] if i < len(examples[key]) else None\n",
        "            else:\n",
        "                example[key] = examples[key]\n",
        "\n",
        "        # Try to find question/problem/instruction and answer/solution/response\n",
        "        user_content = None\n",
        "        assistant_content = None\n",
        "\n",
        "        # Check common field name patterns\n",
        "        for user_key in [\"problem\", \"question\", \"instruction\", \"input\", \"prompt\", \"user\", \"query\"]:\n",
        "            if user_key in example and example[user_key] is not None:\n",
        "                user_content = example[user_key]\n",
        "                break\n",
        "\n",
        "        for assistant_key in [\"generated_solution\", \"solution\", \"answer\", \"response\", \"output\", \"assistant\", \"completion\"]:\n",
        "            if assistant_key in example and example[assistant_key] is not None:\n",
        "                assistant_content = example[assistant_key]\n",
        "                break\n",
        "\n",
        "        # If we have conversations already formatted\n",
        "        if \"conversations\" in example and example[\"conversations\"] is not None:\n",
        "            conversations.append(example[\"conversations\"])\n",
        "        elif user_content and assistant_content:\n",
        "            conversations.append([\n",
        "                {\"role\" : \"user\", \"content\" : str(user_content)},\n",
        "                {\"role\" : \"assistant\", \"content\" : str(assistant_content)},\n",
        "            ])\n",
        "        elif user_content:\n",
        "            # If we only have user content, create a minimal conversation\n",
        "            conversations.append([\n",
        "                {\"role\": \"user\", \"content\": str(user_content)},\n",
        "                {\"role\": \"assistant\", \"content\": \"\"}  # Empty assistant response\n",
        "            ])\n",
        "        elif \"text\" in example and example[\"text\"] is not None:\n",
        "            # If it's already formatted text, we'll handle it separately\n",
        "            conversations.append([{\"role\": \"user\", \"content\": str(example.get(\"text\", \"\"))}])\n",
        "        else:\n",
        "            # Fallback: create a minimal conversation with available data\n",
        "            # Use the first non-None value as content\n",
        "            content = None\n",
        "            for key, value in example.items():\n",
        "                if value is not None and key not in [\"id\", \"index\"]:\n",
        "                    content = str(value)\n",
        "                    break\n",
        "            if content:\n",
        "                conversations.append([\n",
        "                    {\"role\": \"user\", \"content\": content},\n",
        "                    {\"role\": \"assistant\", \"content\": \"\"}\n",
        "                ])\n",
        "            else:\n",
        "                # Last resort: empty conversation\n",
        "                conversations.append([\n",
        "                    {\"role\": \"user\", \"content\": \"\"},\n",
        "                    {\"role\": \"assistant\", \"content\": \"\"}\n",
        "                ])\n",
        "\n",
        "    return {\"conversations\": conversations}\n",
        "\n",
        "# Convert reasoning dataset\n",
        "reasoning_mapped = reasoning_dataset.map(generate_conversation, batched = True, remove_columns=reasoning_dataset.column_names)\n",
        "\n",
        "# Verify the column exists before accessing it\n",
        "print(f\"Columns in reasoning_mapped: {reasoning_mapped.column_names}\")\n",
        "print(f\"Number of examples: {len(reasoning_mapped)}\")\n",
        "\n",
        "# Check if conversations column exists\n",
        "if \"conversations\" in reasoning_mapped.column_names:\n",
        "    reasoning_conversations = tokenizer.apply_chat_template(\n",
        "        list(reasoning_mapped[\"conversations\"]),\n",
        "        tokenize = False,\n",
        "    )\n",
        "else:\n",
        "    raise ValueError(f\"Expected 'conversations' column but found: {reasoning_mapped.column_names}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTexROzQfJn5"
      },
      "source": [
        "Let's preview the first transformed Swahili conversation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkj4c6NrfIz3"
      },
      "outputs": [],
      "source": [
        "reasoning_conversations[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OMhyEXkfM5e"
      },
      "source": [
        "### Prepare Non-Reasoning Swahili Conversational Data\n",
        "\n",
        "Next, we'll prepare the non-reasoning conversational dataset. If we have a separate conversational split, we'll use Unsloth's `standardize_sharegpt` function to format it properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXBFaeQHfSxp"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "\n",
        "if non_reasoning_dataset is not None:\n",
        "    # Standardize the conversational dataset\n",
        "    dataset = standardize_sharegpt(non_reasoning_dataset)\n",
        "    non_reasoning_conversations = tokenizer.apply_chat_template(\n",
        "        list(dataset[\"conversations\"]),\n",
        "        tokenize = False,\n",
        "    )\n",
        "else:\n",
        "    # If no separate conversational dataset, use a subset of reasoning data\n",
        "    # or create empty list (we'll adjust the ratio accordingly)\n",
        "    print(\"ℹ No separate conversational dataset found. Using reasoning dataset for both.\")\n",
        "    non_reasoning_conversations = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9FcosGvfdNr"
      },
      "source": [
        "Preview the first non-reasoning conversation (if available):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7Y0OonwVrWi"
      },
      "outputs": [],
      "source": [
        "if len(non_reasoning_conversations) > 0:\n",
        "    print(\"First non-reasoning conversation:\")\n",
        "    print(non_reasoning_conversations[0][:500] + \"...\" if len(non_reasoning_conversations[0]) > 500 else non_reasoning_conversations[0])\n",
        "else:\n",
        "    print(\"No separate non-reasoning conversations available.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb0hbEekfeqf"
      },
      "outputs": [],
      "source": [
        "non_reasoning_conversations[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_0L18QMfot4"
      },
      "source": [
        "### Dataset Statistics\n",
        "\n",
        "Let's check the size of our Swahili datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unDFuUq1foWj"
      },
      "outputs": [],
      "source": [
        "print(len(reasoning_conversations))\n",
        "print(len(non_reasoning_conversations))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgknnOf7fn3e"
      },
      "source": [
        "### Combine Swahili Datasets with Optimal Ratio\n",
        "\n",
        "We want to balance reasoning capabilities with conversational abilities for Swahili. Let's define a ratio that maintains strong reasoning while ensuring good conversational performance. We'll use 75% reasoning data and 25% conversational data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_szfriCBgCkU"
      },
      "outputs": [],
      "source": [
        "chat_percentage = 0.25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DANuEJA7gL58"
      },
      "source": [
        "### Sample and Balance Swahili Datasets\n",
        "\n",
        "We'll sample the datasets to achieve the desired ratio of reasoning to conversational data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-e0KO9GgFy3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "if len(non_reasoning_conversations) > 0:\n",
        "    non_reasoning_subset = pd.Series(non_reasoning_conversations)\n",
        "    target_size = int(len(reasoning_conversations) * (chat_percentage / (1 - chat_percentage)))\n",
        "\n",
        "    if len(non_reasoning_subset) > target_size:\n",
        "        non_reasoning_subset = non_reasoning_subset.sample(\n",
        "            target_size,\n",
        "            random_state = 2407,\n",
        "        )\n",
        "    else:\n",
        "        print(f\"ℹ Conversational dataset smaller than target. Using all {len(non_reasoning_subset)} examples.\")\n",
        "else:\n",
        "    # If no conversational data, use a subset of reasoning data\n",
        "    print(\"ℹ No separate conversational data. Using subset of reasoning data for diversity.\")\n",
        "    non_reasoning_subset = pd.Series(reasoning_conversations).sample(\n",
        "        min(int(len(reasoning_conversations) * chat_percentage), len(reasoning_conversations)),\n",
        "        random_state = 2407,\n",
        "    )\n",
        "\n",
        "print(f\"Reasoning conversations: {len(reasoning_conversations)}\")\n",
        "print(f\"Conversational subset: {len(non_reasoning_subset)}\")\n",
        "if len(non_reasoning_subset) + len(reasoning_conversations) > 0:\n",
        "    print(f\"Conversational ratio: {len(non_reasoning_subset) / (len(non_reasoning_subset) + len(reasoning_conversations)):.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR-4prS_gVel"
      },
      "source": [
        "### Final Dataset Combination\n",
        "\n",
        "Now let's combine both Swahili datasets and shuffle them for training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfV47_SXgXH4"
      },
      "outputs": [],
      "source": [
        "data = pd.concat([\n",
        "    pd.Series(reasoning_conversations),\n",
        "    pd.Series(non_reasoning_subset)\n",
        "])\n",
        "data.name = \"text\"\n",
        "\n",
        "from datasets import Dataset\n",
        "combined_dataset = Dataset.from_pandas(pd.DataFrame(data))\n",
        "combined_dataset = combined_dataset.shuffle(seed = 3407)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Fine-tune Qwen3 for Swahili\n",
        "\n",
        "Now let's train our model on the Swahili datasets. We'll do 30 steps initially for testing, but you can set `num_train_epochs=1` for a full training run, and set `max_steps=None` to disable step limiting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = combined_dataset,\n",
        "    eval_dataset = None, # Can set up evaluation!\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 30,\n",
        "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.001,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"none\", # Use TrackIO/WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9fa371ShyhB"
      },
      "source": [
        "### Start Swahili Fine-tuning\n",
        "\n",
        "Let's begin training the model on Swahili data! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Test Fine-tuned Model with Swahili Prompts\n",
        "\n",
        "Let's test our fine-tuned model with Swahili prompts! According to the Qwen-3 team, the recommended settings for reasoning inference are `temperature = 0.6, top_p = 0.95, top_k = 20`. For normal chat-based inference, use `temperature = 0.7, top_p = 0.8, top_k = 20`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "# Test with Swahili conversational prompt (non-thinking mode)\n",
        "messages = [\n",
        "    {\"role\" : \"user\", \"content\" : \"Eleza kwa ufupi kuhusu umuhimu wa lugha ya Kiswahili katika Afrika Mashariki na jinsi inavyotumika katika maisha ya kila siku.\"}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    enable_thinking = False, # Disable thinking\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "print(\"=== Fine-tuned Model Response (Swahili - Non-Thinking Mode) ===\")\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 256, # Increase for longer outputs!\n",
        "    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j873RMcEi9uq"
      },
      "outputs": [],
      "source": [
        "# Test with Swahili reasoning prompt (thinking mode enabled)\n",
        "messages = [\n",
        "    {\"role\" : \"user\", \"content\" : \"Kama una shilingi 5000 na unataka kununua vitabu viwili: moja ni shilingi 2000 na nyingine ni shilingi 3500. Je, utabaki na pesa ngapi?\"}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    enable_thinking = True, # Enable thinking for reasoning\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "print(\"\\n=== Fine-tuned Model Response (Swahili - Thinking Mode) ===\")\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 1024, # Increase for longer outputs!\n",
        "    temperature = 0.6, top_p = 0.95, top_k = 20, # For thinking\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Save Fine-tuned Swahili Model\n",
        "\n",
        "To save the final fine-tuned Swahili model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "### Load Saved Swahili Model\n",
        "\n",
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = 2048,\n",
        "        load_in_4bit = True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Save Swahili Model in Different Formats\n",
        "\n",
        "We also support saving to `float16` directly for VLLM deployment. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained(\"model\")\n",
        "    tokenizer.save_pretrained(\"model\")\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub(\"hf/model\", token = \"\")\n",
        "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### Export Swahili Model to GGUF / llama.cpp Format\n",
        "\n",
        "To save your fine-tuned Swahili model to `GGUF` / `llama.cpp` format, we support it natively! We clone `llama.cpp` and default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOfJSxs_VJjz"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp.\n",
        "\n",
        "## Summary\n",
        "\n",
        "We've successfully fine-tuned the Qwen3-14B model for enhanced Swahili cognitive capabilities using the Nadhari/Swahili-Thinking datasets. The model now has improved:\n",
        "- **Text understanding** in Swahili\n",
        "- **Reasoning capabilities** for Swahili problem-solving\n",
        "- **Conversational abilities** in Swahili\n",
        "\n",
        "The fine-tuned model can be deployed for various Swahili language applications across African countries and globally.\n",
        "\n",
        "---\n",
        "\n",
        "## Resources and Support\n",
        "\n",
        "If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other useful links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "4. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
        "</div>\n",
        "\n",
        "  This notebook and all Unsloth notebooks are licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}